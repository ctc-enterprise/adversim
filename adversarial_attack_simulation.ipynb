{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.transforms import functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "_B3_K3qDNbuU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "OUTPUT_DIR = \"attack_results\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Target classes to attack (COCO labels: 1=person, 3=car, 13=stop sign)\n",
        "TARGET_LABELS = [1, 3, 13]"
      ],
      "metadata": {
        "id": "n7mIXl6rNqpE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model():\n",
        "    \"\"\"Loads a pre-trained Faster R-CNN model.\"\"\"\n",
        "    print(f\"Loading Faster R-CNN model on {DEVICE}...\")\n",
        "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    model.to(DEVICE)\n",
        "    model.eval() # Set to eval mode, but we will need gradients for the input\n",
        "    return model\n",
        "\n",
        "def get_sample_image():\n",
        "    \"\"\"Downloads a sample image for testing.\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg\"\n",
        "    # Alternative complex scene:\n",
        "    # url = \"https://upload.wikimedia.org/wikipedia/commons/f/f0/Traffic_jam_in_Delhi.jpg\"\n",
        "\n",
        "    print(f\"Downloading sample image from {url}...\")\n",
        "    response = requests.get(url)\n",
        "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    return img\n",
        "\n",
        "def preprocess(image):\n",
        "    \"\"\"Converts PIL image to Tensor and normalizes.\"\"\"\n",
        "    img_tensor = F.to_tensor(image).to(DEVICE)\n",
        "    return img_tensor\n",
        "\n",
        "def show_prediction(img_tensor, model, title, save_path=None, threshold=0.5):\n",
        "    \"\"\"Runs inference and visualizes predictions.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        predictions = model([img_tensor])[0]\n",
        "\n",
        "    img_np = img_tensor.cpu().permute(1, 2, 0).numpy()\n",
        "    img_np = np.clip(img_np, 0, 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(img_np)\n",
        "\n",
        "    ax = plt.gca()\n",
        "\n",
        "    has_detection = False\n",
        "    for box, label, score in zip(predictions['boxes'], predictions['labels'], predictions['scores']):\n",
        "        if score > threshold:\n",
        "            has_detection = True\n",
        "            box = box.cpu().numpy()\n",
        "            rect = plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1],\n",
        "                                 fill=False, color='red', linewidth=2)\n",
        "            ax.add_patch(rect)\n",
        "            plt.text(box[0], box[1], f\"{score:.2f}\", color='red', fontsize=12, backgroundcolor='white')\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Saved result to {save_path}\")\n",
        "\n",
        "    plt.close()\n",
        "    return has_detection"
      ],
      "metadata": {
        "id": "25AEA5g_NtpT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Digital Attack: PGD (Projected Gradient Descent) ---\n",
        "\n",
        "def pgd_attack(model, img_tensor, epsilon=0.05, alpha=0.01, num_iter=20):\n",
        "    \"\"\"\n",
        "    Performs a digital PGD attack.\n",
        "    Goal: Perturb the WHOLE image slightly to suppress detections.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Digital PGD Attack ---\")\n",
        "\n",
        "    # Clone image and enable gradients\n",
        "    adv_img = img_tensor.clone().detach()\n",
        "    adv_img.requires_grad = True\n",
        "\n",
        "    optimizer = torch.optim.SGD([adv_img], lr=alpha)\n",
        "\n",
        "    for i in tqdm(range(num_iter)):\n",
        "        # Forward pass\n",
        "        # Faster R-CNN returns a loss dictionary during training mode.\n",
        "        # However, we are attacking the inference output.\n",
        "        # Standard trick: We need to maximize the loss of the correct detections\n",
        "        # OR minimize the confidence of the detections.\n",
        "\n",
        "        # For simplicity in this prototype, we force the model to 'train' mode\n",
        "        # momentarily to get loss values for the original ground truth (or pseudo-ground truth).\n",
        "\n",
        "        # 1. Get pseudo-ground truth from clean image\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            clean_preds = model([img_tensor])[0]\n",
        "\n",
        "        # Filter only high confidence targets to suppress\n",
        "        targets = {\n",
        "            'boxes': clean_preds['boxes'][clean_preds['scores'] > 0.5],\n",
        "            'labels': clean_preds['labels'][clean_preds['scores'] > 0.5]\n",
        "        }\n",
        "\n",
        "        if len(targets['boxes']) == 0:\n",
        "            print(\"No objects to attack.\")\n",
        "            break\n",
        "\n",
        "        # 2. Switch to train mode to compute loss gradients\n",
        "        model.train()\n",
        "\n",
        "        # We want to MAXIMIZE this loss (make the model confused)\n",
        "        # FasterRCNN computes loss automatically if targets are provided\n",
        "        loss_dict = model([adv_img], [targets])\n",
        "        total_loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass\n",
        "        total_loss.backward()\n",
        "\n",
        "        # Update image: We want to INCREASE loss, so we ADD gradient\n",
        "        data_grad = adv_img.grad.data\n",
        "        adv_img.data = adv_img.data + alpha * data_grad.sign()\n",
        "\n",
        "        # Projection (Clip noise to epsilon ball)\n",
        "        eta = torch.clamp(adv_img.data - img_tensor.data, -epsilon, epsilon)\n",
        "        adv_img.data = torch.clamp(img_tensor.data + eta, 0, 1)\n",
        "\n",
        "        # Reset gradient for next step\n",
        "        adv_img.grad.data.zero_()\n",
        "\n",
        "    model.eval()\n",
        "    return adv_img.detach()"
      ],
      "metadata": {
        "id": "DeK023AGNxyz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Physical Attack Simulation: Adversarial Patch with EOT ---\n",
        "\n",
        "class AdversarialPatch:\n",
        "    def __init__(self, patch_shape=(3, 100, 100)):\n",
        "        \"\"\"\n",
        "        patch_shape: (Channels, Height, Width)\n",
        "        \"\"\"\n",
        "        # Initialize patch with random noise or gray\n",
        "        self.patch = torch.rand(patch_shape, device=DEVICE, requires_grad=True)\n",
        "        self.patch_shape = patch_shape\n",
        "\n",
        "    def apply_patch(self, img_tensor, patch, location=None):\n",
        "        \"\"\"\n",
        "        Applies the patch to the image.\n",
        "        If location is None, places it randomly (simulating EOT).\n",
        "        \"\"\"\n",
        "        _, h, w = img_tensor.shape\n",
        "        ph, pw = self.patch_shape[1], self.patch_shape[2]\n",
        "\n",
        "        patched_img = img_tensor.clone()\n",
        "\n",
        "        if location is None:\n",
        "            # Random location (EOT aspect)\n",
        "            x = random.randint(0, w - pw)\n",
        "            y = random.randint(0, h - ph)\n",
        "        else:\n",
        "            x, y = location\n",
        "\n",
        "        # Overlay patch\n",
        "        # In a real physical simulator, we would use affine transforms (rotation/perspective) here.\n",
        "        # For this prototype, we do a simple overlay + noise to simulate print imperfections.\n",
        "\n",
        "        # Simulating \"Physical\" noise (printing errors, lighting)\n",
        "        noise = torch.randn_like(patch) * 0.05\n",
        "        robust_patch = torch.clamp(patch + noise, 0, 1)\n",
        "\n",
        "        patched_img[:, y:y+ph, x:x+pw] = robust_patch\n",
        "        return patched_img\n",
        "\n",
        "    def optimize(self, model, ref_img_tensor, epochs=50, lr=0.05):\n",
        "        print(\"\\n--- Starting Physical Patch Optimization (EOT) ---\")\n",
        "\n",
        "        # We optimize ONLY the patch\n",
        "        optimizer = torch.optim.Adam([self.patch], lr=lr)\n",
        "\n",
        "        # Get pseudo-labels to attack\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            initial_preds = model([ref_img_tensor])[0]\n",
        "\n",
        "        # If no objects found, nothing to attack\n",
        "        if len(initial_preds['boxes']) == 0:\n",
        "            print(\"No objects found to attach patch to.\")\n",
        "            return\n",
        "\n",
        "        targets = {\n",
        "            'boxes': initial_preds['boxes'],\n",
        "            'labels': initial_preds['labels']\n",
        "        }\n",
        "\n",
        "        for epoch in tqdm(range(epochs)):\n",
        "            # Expectation Over Transformation (EOT) Loop\n",
        "            # We average gradients over multiple random transformations/placements\n",
        "\n",
        "            acc_loss = 0\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mini-batch of transformations (e.g., 5 random placements per step)\n",
        "            for _ in range(5):\n",
        "                # Apply patch at random location (Transformation)\n",
        "                patched_img = self.apply_patch(ref_img_tensor, self.patch, location=None)\n",
        "\n",
        "                # We need to maximize object loss (make objects disappear)\n",
        "                model.train()\n",
        "                loss_dict = model([patched_img], [targets])\n",
        "\n",
        "                # Losses: classifier loss + box regression loss\n",
        "                # Maximizing these makes the model fail to detect the ground truth\n",
        "                loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "                # Backward\n",
        "                # We want to MAXIMIZE loss, so we minimize -loss\n",
        "                (-loss).backward()\n",
        "                acc_loss += loss.item()\n",
        "\n",
        "            # Update patch\n",
        "            optimizer.step()\n",
        "\n",
        "            # Clamp patch to be a valid image (0-1) - \"Printability\" constraint\n",
        "            self.patch.data.clamp_(0, 1)\n",
        "\n",
        "        model.eval()\n",
        "        print(\"Patch optimization complete.\")"
      ],
      "metadata": {
        "id": "aEyeYRtgN2fs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution ---\n",
        "\n",
        "def main():\n",
        "    # 1. Setup\n",
        "    model = load_model()\n",
        "    img_pil = get_sample_image()\n",
        "    img_tensor = preprocess(img_pil)\n",
        "\n",
        "    # 2. Baseline\n",
        "    print(\"Running baseline inference...\")\n",
        "    show_prediction(img_tensor, model, \"Original Image\",\n",
        "                   save_path=os.path.join(OUTPUT_DIR, \"1_baseline.png\"))\n",
        "\n",
        "    # 3. Digital Attack (PGD)\n",
        "    adv_tensor_pgd = pgd_attack(model, img_tensor, epsilon=0.1, num_iter=30)\n",
        "    print(\"Running inference on Digital Attack...\")\n",
        "    show_prediction(adv_tensor_pgd, model, \"Digital PGD Attack\",\n",
        "                   save_path=os.path.join(OUTPUT_DIR, \"2_digital_attack.png\"))\n",
        "\n",
        "    # Save the noise itself for visualization\n",
        "    noise = (adv_tensor_pgd - img_tensor).abs().cpu().permute(1, 2, 0).numpy() # Amplify for visibility\n",
        "    plt.imsave(os.path.join(OUTPUT_DIR, \"2b_digital_noise_pattern.png\"), np.clip(noise * 10, 0, 1))\n",
        "\n",
        "    # 4. Physical Attack (Adversarial Patch)\n",
        "    # Create a patch roughly 15% of image size\n",
        "    h, w = img_tensor.shape[1], img_tensor.shape[2]\n",
        "    patch_size = int(min(h, w) * 0.2)\n",
        "    attacker = AdversarialPatch(patch_shape=(3, patch_size, patch_size))\n",
        "\n",
        "    # Train the patch\n",
        "    attacker.optimize(model, img_tensor, epochs=20)\n",
        "\n",
        "    # Apply optimized patch to a specific location for final visualization\n",
        "    # (e.g., center of image)\n",
        "    final_patch_img = attacker.apply_patch(img_tensor, attacker.patch, location=(w//2 - patch_size//2, h//2 - patch_size//2))\n",
        "\n",
        "    print(\"Running inference on Physical Patch Attack...\")\n",
        "    show_prediction(final_patch_img.detach(), model, \"Physical Patch Attack\",\n",
        "                   save_path=os.path.join(OUTPUT_DIR, \"3_physical_attack.png\"))\n",
        "\n",
        "    # Save the isolated patch\n",
        "    patch_np = attacker.patch.detach().cpu().permute(1, 2, 0).numpy()\n",
        "    plt.imsave(os.path.join(OUTPUT_DIR, \"3b_generated_patch.png\"), patch_np)\n",
        "\n",
        "    print(\"\\nDone! Check the 'attack_results' folder.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Faster R-CNN model on cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading sample image from https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg...\n",
            "Running baseline inference...\n",
            "Saved result to attack_results/1_baseline.png\n",
            "\n",
            "--- Starting Digital PGD Attack ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:10<00:00,  2.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference on Digital Attack...\n",
            "Saved result to attack_results/2_digital_attack.png\n",
            "\n",
            "--- Starting Physical Patch Optimization (EOT) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:24<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patch optimization complete.\n",
            "Running inference on Physical Patch Attack...\n",
            "Saved result to attack_results/3_physical_attack.png\n",
            "\n",
            "Done! Check the 'attack_results' folder.\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz0CsKtLNZhH",
        "outputId": "3d226dec-e804-4562-9d86-00eef130e218"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}